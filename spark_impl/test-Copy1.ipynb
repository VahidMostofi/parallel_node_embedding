{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/vahid/dev/spark/spark-2.4.4-bin-hadoop2.7/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a129cab00>: Failed to establish a new connection: [Errno 111] Connection refused',))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             )\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1254\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1299\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1300\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1248\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1249\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    973\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 974\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    975\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    185\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             )\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x7f1a129cab00>: Failed to establish a new connection: [Errno 111] Connection refused",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    719\u001b[0m             retries = retries.increment(\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             )\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a129cab00>: Failed to establish a new connection: [Errno 111] Connection refused',))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-06e213c9bea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msend_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'http://127.0.0.1:8080/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msend_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-79-06e213c9bea5>\u001b[0m in \u001b[0;36msend_info\u001b[0;34m(info)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msend_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'http://127.0.0.1:8080/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msend_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/pyspark/lib/python3.6/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='127.0.0.1', port=8080): Max retries exceeded with url: /test (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7f1a129cab00>: Failed to establish a new connection: [Errno 111] Connection refused',))"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "def send_info(info):\n",
    "    requests.get(url='http://127.0.0.1:8080/' + str(info))\n",
    "send_info('test')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "import random\n",
    "# !pip install gensim\n",
    "import networkx as nx\n",
    "import node2vec\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_batches = 64\n",
    "embed_dim = 128\n",
    "number_of_walks = 10 #10\n",
    "length_of_walks = 80 #80\n",
    "node2vec_p = 0.3\n",
    "node2vec_q = 0.3\n",
    "input_path = '/home/vahid/workspace/parallel_node_embedding/data/amazon/'\n",
    "# input_path = '/Users/vahid/Desktop/projects/tesstt/myway/data/arxiv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "# combinations = [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)]\n",
    "combinations = []\n",
    "for i in range(number_of_batches):\n",
    "    for j in range((i+1), number_of_batches):\n",
    "        combinations.append((i,j))\n",
    "\n",
    "# combinations = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (0, 3), (3, 7), (1, 5)]\n",
    "combinations = [(8,53), (4,34), (7,45), (10,62), (0,2), (1,2), (3,26), (8,50), (9,57), (5,39), (6,40), (7,47), (9,55), (0,13), (2,23), (3,27), (5,35), (9,56), (2,24), (0,8), (1,8), (2,8), (3,8), (4,8), (5,8), (6,8), (7,8), (4,33), (0,10), (8,52), (8,51), (10,61), (0,4), (1,4), (2,4), (3,4), (2,21), (5,37), (6,42), (4,30), (6,41), (6,44), (2,22), (3,28), (3,29), (0,6), (1,6), (2,6), (3,6), (4,6), (5,6), (1,15), (1,18), (2,20), (3,25), (5,38), (7,46), (8,54), (10,60), (0,3), (1,3), (2,3), (0,12), (1,16), (4,32), (4,31), (5,36), (6,43), (7,49), (0,1), (0,7), (1,7), (2,7), (3,7), (4,7), (5,7), (6,7), (0,9), (1,9), (2,9), (3,9), (4,9), (5,9), (6,9), (7,9), (8,9), (0,11), (0,14), (1,19), (7,48), (10,63), (0,5), (1,5), (2,5), (3,5), (4,5), (1,17), (9,58), (9,59)]\n",
    "combGraph = nx.Graph()\n",
    "for e in combinations:\n",
    "    combGraph.add_edge(e[0],e[1])\n",
    "print(len(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=test_myway, master=local[*]) created by __init__ at <ipython-input-9-08cc3ceecc5d>:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-08cc3ceecc5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test_myway\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/spark/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \" note this option will be removed in Spark 3.0\")\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/dev/spark/spark-2.4.4-bin-hadoop2.7/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    330\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 332\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    333\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=test_myway, master=local[*]) created by __init__ at <ipython-input-9-08cc3ceecc5d>:1 "
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext(appName=\"test_myway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edge_true = sc.textFile(input_path + \"train_edges_true.txt\") \\\n",
    ".map(lambda x: (int(x.split(' ')[0]), int(x.split(' ')[1]))) \\\n",
    ".map(lambda x: (min(x[0],x[1]), max(x[0],x[1]), 1))\n",
    "train_edge_false= sc.textFile(input_path + \"train_edges_false.txt\") \\\n",
    ".map(lambda x: (int(x.split(' ')[0]), int(x.split(' ')[1]))) \\\n",
    ".map(lambda x: (min(x[0],x[1]), max(x[0],x[1]), 0))\n",
    "train_edges = sc.union([train_edge_true, train_edge_false]).persist()\n",
    "del train_edge_true\n",
    "del train_edge_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = train_edges.flatMap(lambda x: [x[0],x[1]]).distinct().persist()\n",
    "nodes_count = len(nodes.collect()) #todo optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "334863"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 + (nodes_count // number_of_batches)\n",
    "# print('batch_size', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5233"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2parition = nodes.map(lambda x: (x, x // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_node2partition = sc.broadcast(node2parition.collectAsMap())\n",
    "def find_partitions(edge):\n",
    "    def is_in(batch0, batch1, combination):\n",
    "        if batch0 == combination[0] and batch1 == combination[1]:\n",
    "            return True\n",
    "        if batch1 == combination[0] and batch0 == combination[1]:\n",
    "            return True\n",
    "        if batch1 == combination[0] and batch0 == combination[0]:\n",
    "            return True\n",
    "        if batch1 == combination[1] and batch0 == combination[1]:\n",
    "            return True\n",
    "        \n",
    "    batch0 = bd_node2partition.value[edge[0]]\n",
    "    batch1 = bd_node2partition.value[edge[1]]\n",
    "    if batch0 != batch1:\n",
    "        results = [((min(batch0,batch1), max(batch0, batch1)), edge)]\n",
    "    else:\n",
    "        results = []\n",
    "    for comb in combinations:\n",
    "        if is_in(batch0, batch1, comb):\n",
    "            results.append((comb, edge))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edges2comb = train_edges.flatMap(find_partitions)\n",
    "train_edges2comb_filtered = train_edges2comb.filter(lambda ee: ee[0] in combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_edge_list(edge_list):\n",
    "    nx_g = nx.Graph()\n",
    "    for e in edge_list:\n",
    "        if e[2] == 1:\n",
    "            nx_g.add_edge(e[0],e[1])\n",
    "    for edge in nx_g.edges():\n",
    "        nx_g[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "    G = node2vec.Graph(nx_g, False, node2vec_p, node2vec_q)\n",
    "    G.preprocess_transition_probs()\n",
    "    \n",
    "    walks = G.simulate_walks(number_of_walks, length_of_walks, verbose=False)\n",
    "    model = learn_embeddings(walks)\n",
    "    return model\n",
    "\n",
    "def learn_embeddings(walks):\n",
    "    walks = [list(map(str, walk)) for walk in walks]\n",
    "    model = Word2Vec(walks, size=embed_dim, window=10, min_count=0, sg=1, workers=2, iter=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo partition?!!\n",
    "partioned = train_edges2comb_filtered.partitionBy(len(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(x):\n",
    "    comb,iteratable = x\n",
    "    \n",
    "    start_time = time.time()\n",
    "    count = len(iteratable)\n",
    "    model = embed_edge_list(iteratable)\n",
    "    edges_embs = np.zeros([count, embed_dim])\n",
    "    labels = np.zeros([count])\n",
    "    alpha_map = {}\n",
    "    beta_map = {}\n",
    "    i = 0\n",
    "    for e in iteratable:\n",
    "        flag = False\n",
    "        try:\n",
    "            emb1 = model.wv.get_vector(str(e[0]))\n",
    "        except:\n",
    "            flag = True\n",
    "            emb1 = np.zeros([embed_dim])\n",
    "        try:\n",
    "            emb2 = model.wv.get_vector(str(e[1]))\n",
    "        except:\n",
    "            flag = True\n",
    "            emb2 = np.zeros([embed_dim])\n",
    "        edges_embs[i,:] = np.multiply(emb1, emb2)\n",
    "        labels[i] = e[2]\n",
    "        i +=1\n",
    "    \n",
    "    edge_classifier = LogisticRegression(random_state=0, solver='lbfgs', max_iter=250)\n",
    "    edge_classifier.fit(edges_embs, labels)\n",
    "    \n",
    "    for node, batch in bd_node2partition.value.items():\n",
    "        #todo any zeros?!\n",
    "        if batch == comb[0]:\n",
    "            try:\n",
    "                emb1 = model.wv.get_vector(str(node))\n",
    "            except:\n",
    "                emb1 = np.zeros([embed_dim])\n",
    "            alpha_map[node] = emb1\n",
    "        if batch == comb[1]:\n",
    "            try:\n",
    "                emb1 = model.wv.get_vector(str(node))\n",
    "            except:\n",
    "                emb1 = np.zeros([embed_dim])\n",
    "            beta_map[node] = emb1\n",
    "\n",
    "#     send_info('end_make_model_' + str(time.time()) + '_' + str(time.time() - start_time))\n",
    "    return (comb,(edge_classifier, {'alpha_map':alpha_map, 'beta_map': beta_map, 'alpha': comb[0], 'beta':comb[1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = partioned.groupByKey().map(make_model).partitionBy(len(combinations)).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edge_true = sc.textFile(input_path + \"test_edges_true.txt\") \\\n",
    ".map(lambda x: (int(x.split(' ')[0]), int(x.split(' ')[1]))) \\\n",
    ".map(lambda x: (min(x[0],x[1]), max(x[0],x[1]), 1))\n",
    "test_edge_false= sc.textFile(input_path + \"test_edges_false.txt\") \\\n",
    ".map(lambda x: (int(x.split(' ')[0]), int(x.split(' ')[1]))) \\\n",
    ".map(lambda x: (min(x[0],x[1]), max(x[0],x[1]), 0))\n",
    "test_edges = sc.union([test_edge_true, test_edge_false]).persist()\n",
    "del test_edge_true\n",
    "del test_edge_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edges2comb = test_edges.flatMap(find_partitions)\n",
    "# test_edges2comb_filtered = test_edges2comb.filter(lambda ee: ee[0] in combinations)\\\n",
    "# test_edges2comb.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = test_edges2comb.map(lambda x: (x[1], x[0], nx.shortest_path(combGraph, x[0][0], x[0][1])))\n",
    "bb = aa.map(lambda x: (x[0],x[1],len(x[2]), x[2]))\n",
    "dist2 = bb.filter(lambda x: x[2] == 2)\n",
    "dist3 = bb.filter(lambda x: x[2] == 3)\n",
    "dist4 = bb.filter(lambda x: x[2] == 4)\n",
    "# test_edges2comb.map(lambda x: (x[1], x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_first(a,b):\n",
    "    return(min(a,b),max(a,b))\n",
    "# x:(edge, comb, length, _)\n",
    "def f_d2(x):\n",
    "    edge, comb, length, path = x\n",
    "    return (comb, (edge[:2], length, edge))\n",
    "d2 = dist2.map(f_d2).persist()\n",
    "def f_d3(x):\n",
    "    edge, comb, length, path = x\n",
    "    return [(min_first(path[0],path[1]), (edge[:2], length, (edge[0],(edge[1],path[2], path[1], min_first(path[1],path[2])),edge[2]))),\n",
    "            (min_first(path[1],path[2]), (edge[:2], length, ((edge[0],path[0], path[1], min_first(path[0],path[1])), edge[1],edge[2])))]\n",
    "d3 = dist3.flatMap(f_d3).persist()\n",
    "def f_d4(x):\n",
    "    edge, comb, length, path = x\n",
    "    e0 = (edge[0],path[0], path[1], min_first(path[0],path[1]))\n",
    "    e1 = (edge[1],path[3], path[2], min_first(path[2],path[3]))\n",
    "    return (min_first(path[1],path[2]), (edge[:2], length, (e0,e1,edge[2])))\n",
    "d4 = dist4.map(f_d4).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_f(node, current_batch, embding_model):\n",
    "    find_in = None\n",
    "\n",
    "    if current_batch == embding_model['alpha']:\n",
    "        node_vector = embding_model['alpha_map'][node]\n",
    "        find_in = 'beta_map'\n",
    "    if current_batch == embding_model['beta']:\n",
    "        node_vector = embding_model['beta_map'][node]\n",
    "        find_in = 'alpha_map'\n",
    "    best_key = None\n",
    "    distance = 10000\n",
    "    for key,value in embding_model[find_in].items():\n",
    "        dis = np.linalg.norm(value - node_vector) #todo other ways to compare?! cosine\n",
    "        if dis < distance:\n",
    "            distance = dis\n",
    "            best_key = key\n",
    "    return best_key\n",
    "    \n",
    "def d4_left_step(data):\n",
    "    output = []\n",
    "    model_parts = data[1]\n",
    "    classifier_model = model_parts[0]\n",
    "    embding_model = model_parts[1]\n",
    "    for x in data[0]:\n",
    "        comb, (edge, length, (e0,e1,label)) = x\n",
    "        node = e0[0]\n",
    "        current_batch = e0[1]\n",
    "        target_batch = e0[2] #todo check this in ifs below\n",
    "        using_comb = e0[3]\n",
    "        assert using_comb[0] == embding_model['alpha'] and using_comb[1] == embding_model['beta']\n",
    "        A = the_f(node, current_batch, embding_model)\n",
    "\n",
    "        output.append((comb, (edge, length, (A, e1, label))))\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "def d4_right_step(data):\n",
    "    output = []\n",
    "    model_parts = data[1]\n",
    "    classifier_model = model_parts[0]\n",
    "    embding_model = model_parts[1]\n",
    "    for x in data[0]:\n",
    "        comb, (edge, length, (e0,e1,label)) = x\n",
    "        node = e1[0]\n",
    "        current_batch = e1[1]\n",
    "        target_batch = e1[2]\n",
    "        using_comb = e1[3]\n",
    "        assert using_comb[0] == embding_model['alpha'] and using_comb[1] == embding_model['beta']\n",
    "        B = the_f(node, current_batch, embding_model)\n",
    "\n",
    "        output.append((comb, (edge, length, (e0, B, label))))\n",
    "        \n",
    "    return output\n",
    "\n",
    "    \n",
    "d4_left_resolved = d4.map(lambda e: (e[1][2][0][3],e)).partitionBy(len(combinations)).groupByKey(). \\\n",
    "join(models).mapValues(d4_left_step).flatMap(lambda e: e[1])\n",
    "\n",
    "d4_resolved = d4_left_resolved.map(lambda e: (e[1][2][1][3],e)).partitionBy(len(combinations)).groupByKey(). \\\n",
    "join(models).mapValues(d4_right_step).flatMap(lambda e: e[1]).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d4_left_resolved = d4.map(lambda e: (e[1][2][0][3],e)).partitionBy(28).groupByKey().collect()\n",
    "# d4_left_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "d3_left_resolved = d3.filter(lambda e: type(e[1][2][0]) != int).map(lambda e: (e[1][2][0][3],e)). \\\n",
    "partitionBy(len(combinations)).groupByKey().join(models).mapValues(d4_left_step).flatMap(lambda e: e[1])\n",
    "\n",
    "d3_right_resolved= d3.filter(lambda e: type(e[1][2][1]) != int).map(lambda e: (e[1][2][1][3],e)). \\\n",
    "partitionBy(len(combinations)).groupByKey().join(models).mapValues(d4_right_step).flatMap(lambda e: e[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d4_resolved.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo assert, later!\n",
    "# assert len(d3.collect()) == len(d3_right_resolved.collect()) + len(d3_left_resolved.collect())\n",
    "# assert len(d3_right_resolved.collect()) == len(d3_left_resolved.collect())\n",
    "# assert len(d4_resolved.collect()) == len(d4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictables = d2.union(d3_right_resolved).union(d3_left_resolved).union(d4_resolved)\n",
    "predictables = d2.union(d3_right_resolved).union(d3_left_resolved).union(d4_resolved).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictables_grouped = predictables.partitionBy(len(combinations)).groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictables_models = predictables_grouped.join(models).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(values):\n",
    "    test_edges_iter = values[0]\n",
    "    predictable_model = values[1][0]\n",
    "    edge_embeddings = values[1][1] # dictionary with keys: alpha_map, beta_map, alpha, beta\n",
    "    count = len(test_edges_iter)\n",
    "    edges_embs = np.zeros([count, embed_dim])\n",
    "    labels = np.zeros([count])\n",
    "    i = 0\n",
    "    debug_count = 0\n",
    "    names = []\n",
    "    lengths = []\n",
    "    for ee in test_edges_iter:\n",
    "        edge = ee[0]\n",
    "        length = ee[1]\n",
    "        e = ee[2]\n",
    "        \n",
    "        if e[0] in edge_embeddings['alpha_map']:\n",
    "            emb1 = edge_embeddings['alpha_map'][e[0]]\n",
    "        elif e[0] in edge_embeddings['beta_map']:\n",
    "            emb1 = edge_embeddings['beta_map'][e[0]]\n",
    "            \n",
    "        if e[1] in edge_embeddings['alpha_map']:\n",
    "            emb2 = edge_embeddings['alpha_map'][e[1]]\n",
    "        elif e[1] in edge_embeddings['beta_map']:\n",
    "            emb2 = edge_embeddings['beta_map'][e[1]]\n",
    "        \n",
    "        edges_embs[i,:] = np.multiply(emb1, emb2)\n",
    "        labels[i] = e[2]\n",
    "        lengths.append(length)\n",
    "        names.append(str(edge[0]) + '_' + str(edge[1])) \n",
    "        i += 1\n",
    "#     send_info(str(debug_count) + '/' + str(count))\n",
    "    preds = predictable_model.predict_proba(edges_embs)[:, 1]\n",
    "    return (preds, labels, names, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vv = predictables_models.mapValues(predict).persist()\n",
    "outt = vv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(53248,) (53248,)\n",
      "roc_auc_score 0.9561077977073261\n",
      "avg_prc_score 0.9648027574561925\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "all_names = {}\n",
    "all_labels = {}\n",
    "for i in range(0, len(outt)):\n",
    "    names = outt[i][1][2]\n",
    "    for idx, name in enumerate(names):\n",
    "        if name not in all_names:\n",
    "            all_names[name] = []\n",
    "        all_names[name].append(outt[i][1][0][idx])\n",
    "        all_labels[name] = outt[i][1][1][idx]\n",
    "for name in all_labels.keys():\n",
    "    preds.append(np.array(all_names[name]).mean())\n",
    "    labels.append(all_labels[name])\n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)\n",
    "print(preds.shape, labels.shape)\n",
    "test_roc = roc_auc_score(labels, preds)\n",
    "test_ap = average_precision_score(labels, preds)\n",
    "print('roc_auc_score', test_roc)\n",
    "print('avg_prc_score', test_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 7) 0.9320539842239896 11451\n",
      "(0, 1) 0.9431952848887817 8076\n",
      "(1, 2) 0.9540178297167798 10243\n",
      "(0, 3) 0.9426681306985664 8078\n",
      "(6, 7) 0.9548531478525871 8570\n",
      "(2, 3) 0.9451611359574631 10181\n",
      "(5, 6) 0.9559428539240975 11707\n",
      "(4, 5) 0.9400605387168884 9724\n",
      "(1, 5) 0.926380228323281 15061\n",
      "(3, 4) 0.9489234455946335 11471\n",
      "104562\n"
     ]
    }
   ],
   "source": [
    "ss = 0\n",
    "for i in range(len(combinations)):\n",
    "    ss += len(outt[i][1][0])\n",
    "    print(outt[i][0], roc_auc_score(outt[i][1][1], outt[i][1][0]), len(outt[i][1][0]))\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 0.9619950118981477 49726\n",
      "3 0.9302939283972688 49692\n",
      "4 0.8970056059908798 5144\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "ss = 0\n",
    "for l in range(2,5):\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for i in range(len(combinations)):\n",
    "        for j in range(len(outt[i][1][1])):\n",
    "            if outt[i][1][3][j] == l:\n",
    "                labels.append(outt[i][1][1][j])\n",
    "                preds.append(outt[i][1][0][j])\n",
    "    if len(labels) > 0:\n",
    "        print(l, roc_auc_score(labels, preds), len(labels))\n",
    "    else:\n",
    "        print(l, 0)\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
