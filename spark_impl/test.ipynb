{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "def send_info(info):\n",
    "    requests.get(url='http://127.0.0.1:8080/' + str(info))\n",
    "send_info('test')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import time\n",
    "import random\n",
    "\n",
    "import networkx as nx\n",
    "import node2vec\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_batches = 8\n",
    "embed_dim = 128\n",
    "number_of_walks = 10 #10\n",
    "length_of_walks = 80 #80\n",
    "node2vec_p = 0.3\n",
    "node2vec_q = 0.3\n",
    "input_path = '/Users/vahid/Desktop/projects/tesstt/myway/data/facebook/'\n",
    "# input_path = '/Users/vahid/Desktop/projects/tesstt/myway/data/arxiv/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "# combinations = [(0,1),(0,2),(0,3),(1,2),(1,3),(2,3)]\n",
    "combinations = []\n",
    "for i in range(number_of_batches):\n",
    "    for j in range((i+1), number_of_batches):\n",
    "        combinations.append((i,j))\n",
    "\n",
    "combinations = [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6), (6, 7), (0, 3), (3, 7), (1, 5)]\n",
    "combGraph = nx.Graph()\n",
    "for e in combinations:\n",
    "    combGraph.add_edge(e[0],e[1])\n",
    "print(len(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = pyspark.SparkContext(appName=\"test_myway\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edge_true = sc.textFile(input_path + \"train_edges_true.txt\") \\\n",
    ".map(lambda x: (int(x.split(' ')[0]), int(x.split(' ')[1]))) \\\n",
    ".map(lambda x: (min(x[0],x[1]), max(x[0],x[1]), 1))\n",
    "train_edge_false= sc.textFile(input_path + \"train_edges_false.txt\") \\\n",
    ".map(lambda x: (int(x.split(' ')[0]), int(x.split(' ')[1]))) \\\n",
    ".map(lambda x: (min(x[0],x[1]), max(x[0],x[1]), 0))\n",
    "train_edges = sc.union([train_edge_true, train_edge_false]).persist()\n",
    "del train_edge_true\n",
    "del train_edge_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = train_edges.flatMap(lambda x: [x[0],x[1]]).distinct().persist()\n",
    "nodes_count = len(nodes.collect()) #todo optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1 + (nodes_count // number_of_batches)\n",
    "# print('batch_size', batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node2parition = nodes.map(lambda x: (x, x // batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bd_node2partition = sc.broadcast(node2parition.collectAsMap())\n",
    "def find_partitions(edge):\n",
    "    def is_in(batch0, batch1, combination):\n",
    "        if batch0 == combination[0] and batch1 == combination[1]:\n",
    "            return True\n",
    "        if batch1 == combination[0] and batch0 == combination[1]:\n",
    "            return True\n",
    "        if batch1 == combination[0] and batch0 == combination[0]:\n",
    "            return True\n",
    "        if batch1 == combination[1] and batch0 == combination[1]:\n",
    "            return True\n",
    "        \n",
    "    batch0 = bd_node2partition.value[edge[0]]\n",
    "    batch1 = bd_node2partition.value[edge[1]]\n",
    "    if batch0 != batch1:\n",
    "        results = [((min(batch0,batch1), max(batch0, batch1)), edge)]\n",
    "    else:\n",
    "        results = []\n",
    "    for comb in combinations:\n",
    "        if is_in(batch0, batch1, comb):\n",
    "            results.append((comb, edge))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edges2comb = train_edges.flatMap(find_partitions)\n",
    "train_edges2comb_filtered = train_edges2comb.filter(lambda ee: ee[0] in combinations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_edge_list(edge_list):\n",
    "    nx_g = nx.Graph()\n",
    "    for e in edge_list:\n",
    "        if e[2] == 1:\n",
    "            nx_g.add_edge(e[0],e[1])\n",
    "    for edge in nx_g.edges():\n",
    "        nx_g[edge[0]][edge[1]]['weight'] = 1\n",
    "\n",
    "    G = node2vec.Graph(nx_g, False, node2vec_p, node2vec_q)\n",
    "    G.preprocess_transition_probs()\n",
    "    \n",
    "    walks = G.simulate_walks(number_of_walks, length_of_walks, verbose=False)\n",
    "    model = learn_embeddings(walks)\n",
    "    return model\n",
    "\n",
    "def learn_embeddings(walks):\n",
    "    walks = [list(map(str, walk)) for walk in walks]\n",
    "    model = Word2Vec(walks, size=embed_dim, window=10, min_count=0, sg=1, workers=2, iter=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo partition?!!\n",
    "partioned = train_edges2comb_filtered.partitionBy(len(combinations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(x):\n",
    "    comb,iteratable = x\n",
    "    \n",
    "    start_time = time.time()\n",
    "    count = len(iteratable)\n",
    "    model = embed_edge_list(iteratable)\n",
    "    edges_embs = np.zeros([count, embed_dim])\n",
    "    labels = np.zeros([count])\n",
    "    alpha_map = {}\n",
    "    beta_map = {}\n",
    "    i = 0\n",
    "    for e in iteratable:\n",
    "        flag = False\n",
    "        try:\n",
    "            emb1 = model.wv.get_vector(str(e[0]))\n",
    "        except:\n",
    "            flag = True\n",
    "            emb1 = np.zeros([embed_dim])\n",
    "        try:\n",
    "            emb2 = model.wv.get_vector(str(e[1]))\n",
    "        except:\n",
    "            flag = True\n",
    "            emb2 = np.zeros([embed_dim])\n",
    "        edges_embs[i,:] = np.multiply(emb1, emb2)\n",
    "        labels[i] = e[2]\n",
    "        i +=1\n",
    "    \n",
    "    edge_classifier = LogisticRegression(random_state=0, solver='lbfgs', max_iter=250)\n",
    "    edge_classifier.fit(edges_embs, labels)\n",
    "    \n",
    "    for node, batch in bd_node2partition.value.items():\n",
    "        #todo any zeros?!\n",
    "        emb1 = model.wv.get_vector(str(node))\n",
    "        if batch == comb[0]:\n",
    "            alpha_map[node] = emb1\n",
    "        if batch == comb[1]:\n",
    "            #todo any zeros?!\n",
    "            beta_map[node] = emb1\n",
    "\n",
    "#     send_info('end_make_model_' + str(time.time()) + '_' + str(time.time() - start_time))\n",
    "    return (comb,(edge_classifier, {'alpha_map':alpha_map, 'beta_map': beta_map, 'alpha': comb[0], 'beta':comb[1]}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = partioned.groupByKey().map(make_model).partitionBy(len(combinations)).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edge_true = sc.textFile(input_path + \"test_edges_true.txt\") \\\n",
    ".map(lambda x: (int(x.split(' ')[0]), int(x.split(' ')[1]))) \\\n",
    ".map(lambda x: (min(x[0],x[1]), max(x[0],x[1]), 1))\n",
    "test_edge_false= sc.textFile(input_path + \"test_edges_false.txt\") \\\n",
    ".map(lambda x: (int(x.split(' ')[0]), int(x.split(' ')[1]))) \\\n",
    ".map(lambda x: (min(x[0],x[1]), max(x[0],x[1]), 0))\n",
    "test_edges = sc.union([test_edge_true, test_edge_false]).persist()\n",
    "del test_edge_true\n",
    "del test_edge_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edges2comb = test_edges.flatMap(find_partitions)\n",
    "# test_edges2comb_filtered = test_edges2comb.filter(lambda ee: ee[0] in combinations)\\\n",
    "test_edges2comb.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = test_edges2comb.map(lambda x: (x[1], x[0], nx.shortest_path(combGraph, x[0][0], x[0][1])))\n",
    "bb = aa.map(lambda x: (x[0],x[1],len(x[2]), x[2]))\n",
    "dist2 = bb.filter(lambda x: x[2] == 2)\n",
    "dist3 = bb.filter(lambda x: x[2] == 3)\n",
    "dist4 = bb.filter(lambda x: x[2] == 4)\n",
    "# test_edges2comb.map(lambda x: (x[1], x[0])).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dist4.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_first(a,b):\n",
    "    return(min(a,b),max(a,b))\n",
    "# x:(edge, comb, length, _)\n",
    "def f_d2(x):\n",
    "    edge, comb, length, path = x\n",
    "    return (comb, (edge[:2], length, edge))\n",
    "d2 = dist2.map(f_d2).persist()\n",
    "def f_d3(x):\n",
    "    edge, comb, length, path = x\n",
    "    return [(min_first(path[0],path[1]), (edge[:2], length, (edge[0],(edge[1],path[2], path[1], min_first(path[1],path[2])),edge[2]))),\n",
    "            (min_first(path[1],path[2]), (edge[:2], length, ((edge[0],path[0], path[1], min_first(path[0],path[1])), edge[1],edge[2])))]\n",
    "d3 = dist3.flatMap(f_d3).persist()\n",
    "def f_d4(x):\n",
    "    edge, comb, length, path = x\n",
    "    e0 = (edge[0],path[0], path[1], min_first(path[0],path[1]))\n",
    "    e1 = (edge[1],path[3], path[2], min_first(path[2],path[3]))\n",
    "    return (min_first(path[1],path[2]), (edge[:2], length, (e0,e1,edge[2])))\n",
    "d4 = dist4.map(f_d4).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def the_f(node, current_batch, embding_model):\n",
    "    find_in = None\n",
    "\n",
    "    if current_batch == embding_model['alpha']:\n",
    "        node_vector = embding_model['alpha_map'][node]\n",
    "        find_in = 'beta_map'\n",
    "    if current_batch == embding_model['beta']:\n",
    "        node_vector = embding_model['beta_map'][node]\n",
    "        find_in = 'alpha_map'\n",
    "    best_key = None\n",
    "    distance = 10000\n",
    "    for key,value in embding_model[find_in].items():\n",
    "        dis = np.linalg.norm(value - node_vector) #todo other ways to compare?! cosine\n",
    "        if dis < distance:\n",
    "            distance = dis\n",
    "            best_key = key\n",
    "    return best_key\n",
    "    \n",
    "def d4_left_step(data):\n",
    "    output = []\n",
    "    model_parts = data[1]\n",
    "    classifier_model = model_parts[0]\n",
    "    embding_model = model_parts[1]\n",
    "    for x in data[0]:\n",
    "        comb, (edge, length, (e0,e1,label)) = x\n",
    "        node = e0[0]\n",
    "        current_batch = e0[1]\n",
    "        target_batch = e0[2] #todo check this in ifs below\n",
    "        using_comb = e0[3]\n",
    "        assert using_comb[0] == embding_model['alpha'] and using_comb[1] == embding_model['beta']\n",
    "        A = the_f(node, current_batch, embding_model)\n",
    "\n",
    "        output.append((comb, (edge, length, (A, e1, label))))\n",
    "        \n",
    "    return output\n",
    "\n",
    "\n",
    "def d4_right_step(data):\n",
    "    output = []\n",
    "    model_parts = data[1]\n",
    "    classifier_model = model_parts[0]\n",
    "    embding_model = model_parts[1]\n",
    "    for x in data[0]:\n",
    "        comb, (edge, length, (e0,e1,label)) = x\n",
    "        node = e1[0]\n",
    "        current_batch = e1[1]\n",
    "        target_batch = e1[2]\n",
    "        using_comb = e1[3]\n",
    "        assert using_comb[0] == embding_model['alpha'] and using_comb[1] == embding_model['beta']\n",
    "        B = the_f(node, current_batch, embding_model)\n",
    "\n",
    "        output.append((comb, (edge, length, (e0, B, label))))\n",
    "        \n",
    "    return output\n",
    "\n",
    "    \n",
    "d4_left_resolved = d4.map(lambda e: (e[1][2][0][3],e)).partitionBy(len(combinations)).groupByKey(). \\\n",
    "join(models).mapValues(d4_left_step).flatMap(lambda e: e[1])\n",
    "\n",
    "d4_resolved = d4_left_resolved.map(lambda e: (e[1][2][1][3],e)).partitionBy(len(combinations)).groupByKey(). \\\n",
    "join(models).mapValues(d4_right_step).flatMap(lambda e: e[1]).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d4_left_resolved = d4.map(lambda e: (e[1][2][0][3],e)).partitionBy(28).groupByKey().collect()\n",
    "# d4_left_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "d3_left_resolved = d3.filter(lambda e: type(e[1][2][0]) != int).map(lambda e: (e[1][2][0][3],e)). \\\n",
    "partitionBy(len(combinations)).groupByKey().join(models).mapValues(d4_left_step).flatMap(lambda e: e[1])\n",
    "\n",
    "d3_right_resolved= d3.filter(lambda e: type(e[1][2][1]) != int).map(lambda e: (e[1][2][1][3],e)). \\\n",
    "partitionBy(len(combinations)).groupByKey().join(models).mapValues(d4_right_step).flatMap(lambda e: e[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d4_resolved.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#todo assert, later!\n",
    "# assert len(d3.collect()) == len(d3_right_resolved.collect()) + len(d3_left_resolved.collect())\n",
    "# assert len(d3_right_resolved.collect()) == len(d3_left_resolved.collect())\n",
    "# assert len(d4_resolved.collect()) == len(d4.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictables = d2.union(d3_right_resolved).union(d3_left_resolved).union(d4_resolved)\n",
    "predictables = d2.union(d3_right_resolved).union(d3_left_resolved).union(d4_resolved).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictables_grouped = predictables.partitionBy(len(combinations)).groupByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictables_models = predictables_grouped.join(models).persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(values):\n",
    "    test_edges_iter = values[0]\n",
    "    predictable_model = values[1][0]\n",
    "    edge_embeddings = values[1][1] # dictionary with keys: alpha_map, beta_map, alpha, beta\n",
    "    count = len(test_edges_iter)\n",
    "    edges_embs = np.zeros([count, embed_dim])\n",
    "    labels = np.zeros([count])\n",
    "    i = 0\n",
    "    debug_count = 0\n",
    "    names = []\n",
    "    lengths = []\n",
    "    for ee in test_edges_iter:\n",
    "        edge = ee[0]\n",
    "        length = ee[1]\n",
    "        e = ee[2]\n",
    "        \n",
    "        if e[0] in edge_embeddings['alpha_map']:\n",
    "            emb1 = edge_embeddings['alpha_map'][e[0]]\n",
    "        elif e[0] in edge_embeddings['beta_map']:\n",
    "            emb1 = edge_embeddings['beta_map'][e[0]]\n",
    "            \n",
    "        if e[1] in edge_embeddings['alpha_map']:\n",
    "            emb2 = edge_embeddings['alpha_map'][e[1]]\n",
    "        elif e[1] in edge_embeddings['beta_map']:\n",
    "            emb2 = edge_embeddings['beta_map'][e[1]]\n",
    "        \n",
    "        edges_embs[i,:] = np.multiply(emb1, emb2)\n",
    "        labels[i] = e[2]\n",
    "        lengths.append(length)\n",
    "        names.append(str(edge[0]) + '_' + str(edge[1])) \n",
    "        i += 1\n",
    "#     send_info(str(debug_count) + '/' + str(count))\n",
    "    preds = predictable_model.predict_proba(edges_embs)[:, 1]\n",
    "    return (preds, labels, names, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "vv = predictables_models.mapValues(predict).persist()\n",
    "outt = vv.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "all_names = {}\n",
    "all_labels = {}\n",
    "for i in range(0, len(outt)):\n",
    "    names = outt[i][1][2]\n",
    "    for idx, name in enumerate(names):\n",
    "        if name not in all_names:\n",
    "            all_names[name] = []\n",
    "        all_names[name].append(outt[i][1][0][idx])\n",
    "        all_labels[name] = outt[i][1][1][idx]\n",
    "for name in all_labels.keys():\n",
    "    preds.append(np.array(all_names[name]).mean())\n",
    "    labels.append(all_labels[name])\n",
    "preds = np.array(preds)\n",
    "labels = np.array(labels)\n",
    "print(preds.shape, labels.shape)\n",
    "test_roc = roc_auc_score(labels, preds)\n",
    "test_ap = average_precision_score(labels, preds)\n",
    "print('roc_auc_score', test_roc)\n",
    "print('avg_prc_score', test_ap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = 0\n",
    "for i in range(len(combinations)):\n",
    "    ss += len(outt[i][1][0])\n",
    "    print(outt[i][0], roc_auc_score(outt[i][1][1], outt[i][1][0]), len(outt[i][1][0]))\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = 0\n",
    "for l in range(2,5):\n",
    "    labels = []\n",
    "    preds = []\n",
    "    for i in range(len(combinations)):\n",
    "        for j in range(len(outt[i][1][1])):\n",
    "            if outt[i][1][3][j] == l:\n",
    "                labels.append(outt[i][1][1][j])\n",
    "                preds.append(outt[i][1][0][j])\n",
    "    if len(labels) > 0:\n",
    "        print(l, roc_auc_score(labels, preds), len(labels))\n",
    "    else:\n",
    "        print(l, 0)\n",
    "print(ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
